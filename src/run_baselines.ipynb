{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.linear_model import Ridge, LogisticRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error as mse, roc_auc_score as roc, accuracy_score as acc, log_loss\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data.dataset import TextResponseDataset\n",
    "import causal_attribution\n",
    "import util\n",
    "from scipy.sparse import csr_matrix\n",
    "from importlib import reload\n",
    "import data.dataset as ds\n",
    "from model.topic_model import TopicModel\n",
    "from model.model_trainer import ModelTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "from evaluation.evaluator import Evaluator\n",
    "import itertools as it\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation(features, labels, num_documents, n_cv=5, n_folds=10, label_is_bool=False, C=None):\n",
    "    n_metrics = 1 if not label_is_bool else 3\n",
    "    split_indices = util.cross_val_splits(num_documents)\n",
    "    all_indices = np.arange(num_documents)\n",
    "    mses = np.zeros((n_cv,n_metrics))\n",
    "    \n",
    "    if label_is_bool:\n",
    "        if C is not None:\n",
    "            model = LogisticRegression(C=C, penalty='l1', solver='liblinear')\n",
    "        else:\n",
    "            model = LogisticRegression(solver='liblinear')\n",
    "    else:\n",
    "        model = Ridge() #Lasso(alpha=C)#\n",
    "    for i in range(n_cv):\n",
    "        te_indices = split_indices[i]\n",
    "        tr_indices = np.setdiff1d(all_indices, te_indices)\n",
    "\n",
    "        tr_feat = features[tr_indices, :]\n",
    "        tr_labels = labels[tr_indices]\n",
    "        te_feat = features[te_indices,:]\n",
    "        te_labels = labels[te_indices]\n",
    "        \n",
    "        model.fit(tr_feat, tr_labels)\n",
    "        \n",
    "        te_pred = model.predict(te_feat)\n",
    "        if label_is_bool:\n",
    "            te_pr_pred = model.predict_proba(te_feat)[:,1]\n",
    "            ll = log_loss(te_labels, te_pr_pred)\n",
    "            auc = roc(te_labels, te_pr_pred)\n",
    "            accuracy = acc(te_labels, te_pred)\n",
    "            mses[i][0] = auc\n",
    "            mses[i][1] = ll\n",
    "            mses[i][2] = accuracy\n",
    "        else:\n",
    "            err = mse(te_labels, te_pred)\n",
    "            mses[i][0] = err\n",
    "    \n",
    "    return mses.mean(axis=0), mses.std(axis=0)\n",
    "\n",
    "\n",
    "def interpret_model(features, labels, num_documents, split=0, label_is_bool=True):\n",
    "    n_metrics = 1 if not label_is_bool else 3\n",
    "    split_indices = util.cross_val_splits(num_documents)\n",
    "    all_indices = np.arange(num_documents)\n",
    "    if label_is_bool:\n",
    "            model = LogisticRegression(solver='liblinear')\n",
    "    else:\n",
    "        model = Ridge()\n",
    "    i = split\n",
    "    te_indices = split_indices[i]\n",
    "    tr_indices = np.setdiff1d(all_indices, te_indices)\n",
    "\n",
    "    tr_feat = features[tr_indices, :]\n",
    "    tr_labels = labels[tr_indices]\n",
    "    te_feat = features[te_indices,:]\n",
    "    te_labels = labels[te_indices]\n",
    "\n",
    "    model.fit(tr_feat, tr_labels)\n",
    "    return model.coef_\n",
    "\n",
    "\n",
    "def get_normalized_pmi(topics, counts, num_words=10):\n",
    "    num_topics = topics.shape[0]\n",
    "    num_docs = counts.shape[0]\n",
    "    per_topic_npmi = np.zeros(num_topics)\n",
    "\n",
    "    bin_counts = counts.copy()\n",
    "    bin_counts[bin_counts>1] = 1\n",
    "    \n",
    "    tf = csr_matrix(bin_counts)\n",
    "    cooccurence = tf.T.dot(tf)\n",
    "    cooccurence = cooccurence.toarray()\n",
    "\n",
    "    doc_count = bin_counts.sum(axis=0)\n",
    "    prob = doc_count/num_docs\n",
    "    cooccurence_prob = cooccurence/num_docs\n",
    "\n",
    "    for k in range(num_topics):\n",
    "        npmi_total = 0\n",
    "        beta = topics[k,:]\n",
    "        top_words = (-beta).argsort()[:num_words]\n",
    "        n = 0 \n",
    "        for (w1, w2) in it.combinations(top_words, 2):\n",
    "            joint = cooccurence_prob[w1][w2]+1e-7\n",
    "            p_w1 = prob[w1]+1e-7\n",
    "            p_w2 = prob[w2]+1e-7\n",
    "            numerator = np.log(joint/(p_w1*p_w2))\n",
    "            denom = -np.log(joint)\n",
    "            npmi_total += numerator/denom\n",
    "            n+=1\n",
    "        per_topic_npmi[k] = npmi_total\n",
    "    return per_topic_npmi.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the dataset name (and topic, for Media Framing Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'peerread'\n",
    "framing_topic = 'guncontrol'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 11778, 5683)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ds)\n",
    "\n",
    "label_is_bool=False\n",
    "\n",
    "if dataset in TextResponseDataset.CLASSIFICATION_SETTINGS:\n",
    "    label_is_bool=True\n",
    "\n",
    "if dataset == 'amazon':\n",
    "    datafile = '../dat/reviews_Office_Products_5.json'\n",
    "elif dataset == 'amazon_binary':\n",
    "    datafile = '../dat/reviews_Grocery_and_Gourmet_Food_5.json'\n",
    "elif dataset == 'yelp':\n",
    "    datafile = '../dat/yelp_review_polarity_csv/train.csv'\n",
    "elif dataset == 'peerread':\n",
    "    datafile = '../dat/peerread_abstracts.csv'\n",
    "elif dataset == 'framing_corpus':\n",
    "    datafile = '../dat/framing/'\n",
    "else:\n",
    "    datafile = '../dat/cs_papers.gz'\n",
    "\n",
    "if dataset == 'framing_corpus':\n",
    "    proc_file = '../dat/proc/' + dataset + '_' + framing_topic + '_proc.npz'\n",
    "else:\n",
    "    proc_file = '../dat/proc/' + dataset + '_proc.npz'\n",
    "\n",
    "components = {'amazon':30, \n",
    "              'semantic_scholar':50, \n",
    "              'peerread':50, 'yelp':30, \n",
    "              'amazon_binary':20, \n",
    "              'framing_corpus':10\n",
    "             }\n",
    "text_dataset = ds.TextResponseDataset(dataset, \n",
    "                                      datafile, \n",
    "                                      proc_file, \n",
    "                                      use_bigrams=False,\n",
    "                                      framing_topic=framing_topic)\n",
    "text_dataset.process_dataset()\n",
    "text_dataset.preprocessing()\n",
    "\n",
    "counts = text_dataset.counts\n",
    "labels= text_dataset.labels\n",
    "vocab= text_dataset.vocab\n",
    "docs = text_dataset.docs\n",
    "\n",
    "n_components=components[dataset]\n",
    "num_documents = counts.shape[0]\n",
    "n_components, num_documents, counts.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running PCA on cooccurence matrix of words to create embeddings of words for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.69332   , 0.56887495, 0.69879518]),\n",
       " array([0.02759584, 0.01027946, 0.00647571]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = csr_matrix(counts)\n",
    "cooccurence = tf.T.dot(tf)\n",
    "cooccurence = cooccurence.toarray()\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "embeddings = pca.fit_transform(cooccurence)\n",
    "\n",
    "features = np.zeros((num_documents, n_components))\n",
    "for i in range(num_documents):\n",
    "    tf = counts[i,:]\n",
    "    nonzero = (tf > 0)\n",
    "    features[i] = embeddings[nonzero,:].sum(axis=0)\n",
    "\n",
    "result_pca = run_cross_validation(features, labels, num_documents, label_is_bool=label_is_bool)\n",
    "result_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA features for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved results...\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'framing_corpus':\n",
    "    pretraining_file = '../dat/proc/' + dataset + '_' + framing_topic + '_pretraining.npz'\n",
    "else:\n",
    "    pretraining_file = '../dat/proc/' + dataset + '_pretraining.npz'\n",
    "    \n",
    "if os.path.exists(pretraining_file):\n",
    "    print(\"Loading saved results...\")\n",
    "    arr = np.load(pretraining_file)\n",
    "    doc_rep = arr['theta'] \n",
    "    topics = arr['beta']\n",
    "    print(\"Completed.\")\n",
    "else:\n",
    "    lda_model = LDA(n_components=n_components)\n",
    "    doc_rep = lda_model.fit_transform(counts)\n",
    "    \n",
    "    unnormalized_topics = lda_model.components_\n",
    "    topics = lda_model.components_ / lda_model.components_.sum(axis=1)[:,np.newaxis]\n",
    "    \n",
    "    if dataset == 'framing_corpus':\n",
    "        pretrained_out_file = '../dat/proc/' + dataset + '_' + framing_topic + '_pretraining'\n",
    "    else:\n",
    "        pretrained_out_file = '../dat/proc/' + dataset + '_pretraining'\n",
    "\n",
    "    np.savez_compressed(pretrained_out_file, theta=doc_rep, beta=topics)\n",
    "    \n",
    "    print(\"Perplexity:\", lda_model.perplexity(counts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['system', 'used', 'based', 'paper', 'proposed', 'traffic', 'area']\n",
      "Topic 1: ['network', 'neural', 'memory', 'neural network', 'gradient', 'training', 'new']\n",
      "Topic 2: ['topic', 'rule', 'model', 'probabilistic', 'algorithm', 'distribution', 'inference']\n",
      "Topic 3: ['system', 'knowledge', 'agent', 'dialogue', 'domain', 'argument', 'paper']\n",
      "Topic 4: ['information', 'extraction', 'social', 'network', 'rule', 'social network', 'decision']\n",
      "Topic 5: ['model', 'sequence', 'neural', 'sentence', 'attention', 'recurrent', 'task']\n",
      "Topic 6: ['question', 'task', 'model', 'word', 'answer', 'semantic', 'entity']\n",
      "Topic 7: ['detection', 'detect', 'anomaly', 'algorithm', 'system', 'activity', 'tracking']\n",
      "Topic 8: ['model', 'human', 'role', 'attribute', 'play', 'qualitative', 'reasoning']\n",
      "Topic 9: ['learning', 'task', 'label', 'sample', 'generalization', 'learner', 'complexity']\n",
      "Topic 10: ['ensemble', 'model', 'concept', 'summarization', 'two', 'result', 'state art']\n",
      "Topic 11: ['translation', 'language', 'machine', 'machine translation', 'model', 'system', 'source']\n",
      "Topic 12: ['student', 'using', 'sensor', 'system', 'data', 'course', 'time']\n",
      "Topic 13: ['user', 'system', 'item', 'recommendation', 'attack', 'preference', 'filtering']\n",
      "Topic 14: ['learning', 'task', 'approach', 'language', 'rnn', 'using', 'recurrent']\n",
      "Topic 15: ['model', 'approach', 'parser', 'dependency', 'using', 'corpus', 'parsing']\n",
      "Topic 16: ['data', 'learning', 'machine', 'machine learning', 'training', 'set', 'large']\n",
      "Topic 17: ['social', 'medium', 'social medium', 'emotion', 'music', 'message', 'user']\n",
      "Topic 18: ['time', 'series', 'fuzzy', 'time series', 'automaton', 'system', 'process']\n",
      "Topic 19: ['causal', 'software', 'cause', 'signal', 'system', 'data', 'application']\n",
      "Topic 20: ['measure', 'matrix', 'metric', 'function', 'distance', 'similarity', 'problem']\n",
      "Topic 21: ['method', 'learning', 'performance', 'training', 'propose', 'show', 'new']\n",
      "Topic 22: ['review', 'entity', 'opinion', 'news', 'product', 'twitter', 'tweet']\n",
      "Topic 23: ['ontology', 'web', 'semantic', 'knowledge', 'paper', 'approach', 'research']\n",
      "Topic 24: ['network', 'neural', 'neural network', 'convolutional', 'speech', 'recognition', 'layer']\n",
      "Topic 25: ['graph', 'logic', 'set', 'semantics', 'knowledge', 'operator', 'theory']\n",
      "Topic 26: ['game', 'intelligence', 'ai', 'artificial', 'artificial intelligence', 'player', 'privacy']\n",
      "Topic 27: ['reasoning', 'carlo', 'given', 'chain', 'monte', 'monte carlo', 'variable']\n",
      "Topic 28: ['query', 'search', 'data', 'ha', 'big', 'algorithm', 'implementation']\n",
      "Topic 29: ['human', 'robot', 'interaction', 'cognitive', 'motion', 'perception', 'trajectory']\n",
      "Topic 30: ['network', 'probability', 'bayesian', 'node', 'algorithm', 'belief', 'inference']\n",
      "Topic 31: ['algorithm', 'problem', 'bound', 'optimization', 'function', 'stochastic', 'gradient']\n",
      "Topic 32: ['speech', 'recognition', 'learning', 'system', 'deep', 'deep learning', 'training']\n",
      "Topic 33: ['program', 'et', 'al', 'et al', 'language', 'model', 'discourse']\n",
      "Topic 34: ['model', 'inference', 'approach', 'process', 'modeling', 'variational', 'bayesian']\n",
      "Topic 35: ['method', 'distribution', 'variable', 'sparse', 'estimation', 'model', 'random']\n",
      "Topic 36: ['task', 'assignment', 'problem', 'worker', 'x', 'crowdsourcing', 'heuristic']\n",
      "Topic 37: ['data', 'clustering', 'event', 'cluster', 'pattern', 'algorithm', 'mining']\n",
      "Topic 38: ['feature', 'classification', 'analysis', 'sentiment', 'classifier', 'method', 'data']\n",
      "Topic 39: ['network', 'deep', 'neural', 'neural network', 'learning', 'deep learning', 'architecture']\n",
      "Topic 40: ['word', 'representation', 'vector', 'embeddings', 'embedding', 'dictionary', 'language']\n",
      "Topic 41: ['problem', 'solution', 'algorithm', 'decision', 'optimization', 'approach', 'preference']\n",
      "Topic 42: ['model', 'data', 'network', 'training', 'learning', 'generative', 'show']\n",
      "Topic 43: ['image', 'object', 'visual', 'feature', 'video', 'vision', 'representation']\n",
      "Topic 44: ['agent', 'policy', 'action', 'state', 'planning', 'problem', 'decision']\n",
      "Topic 45: ['problem', 'algorithm', 'constraint', 'show', 'set', 'number', 'result']\n",
      "Topic 46: ['kernel', 'space', 'tensor', 'matrix', 'weight', 'approximation', 'binary']\n",
      "Topic 47: ['document', 'relation', 'ranking', 'information', 'retrieval', 'text', 'term']\n",
      "Topic 48: ['language', 'natural', 'natural language', 'processing', 'text', 'language processing', 'task']\n",
      "Topic 49: ['learning', 'algorithm', 'loss', 'tree', 'framework', 'structured', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_components):\n",
    "    beta = topics[k]\n",
    "    top_words = (-beta).argsort()[:7]\n",
    "    topic_words = [(vocab[t]) for t in top_words]\n",
    "    print('Topic {}: {}'.format(k, topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.80188297, 0.44200753, 0.78742566]),\n",
       " array([0.01562673, 0.00534001, 0.00538419]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lda = run_cross_validation(doc_rep, labels, num_documents, label_is_bool=label_is_bool)\n",
    "results_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW features for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = counts/counts.sum(axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.78269998, 0.5118064 , 0.75904843]),\n",
       " array([0.01010406, 0.01224703, 0.01284694]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_bow = run_cross_validation(csr_matrix(normalized), labels, num_documents, label_is_bool=label_is_bool)\n",
    "result_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression adjusted for topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81205147 0.43660107 0.78419711]\n",
      " [0.81887834 0.44377002 0.78589635]\n",
      " [0.77988073 0.43994415 0.7884452 ]\n",
      " [0.80625635 0.43761526 0.78759558]\n",
      " [0.82464898 0.42720401 0.79694138]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.80834317, 0.4370269 , 0.78861512]),\n",
       " array([0.01552473, 0.00549539, 0.00441147]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.column_stack((normalized,doc_rep))\n",
    "result_adjusted = run_cross_validation(csr_matrix(features), labels, num_documents, label_is_bool=label_is_bool)\n",
    "result_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
